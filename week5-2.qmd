---
title: "Week5: Clustering and PCA"
format:
  html:
    toc: true
    number-sections: true
  pdf:
    toc: true
    number-sections: true
execute:
  warning: false
  message: false
  error: false
---

## Loading Data {.unnumbered}

We reuse the same prepared data introduced earlier.

### R: Loading Data and Creating Splits {.unnumbered}

```{r}
library(tidyverse)
library(dplyr)

bank <- read.csv("data/raw/bank-additional.csv", sep = ";", stringsAsFactors = FALSE)


```

### Python: Loading Data and Creating Splits {.unnumbered}

```{python}
import pandas as pd
from sklearn.compose import ColumnTransformer

bank = pd.read_csv("data/raw/bank-additional.csv", sep=";")

```

---

---

---

## From Prediction to Structure

So far in this module, most of our work has been **supervised**:

- we had a target variable,
- we trained models to predict it,
- we evaluated performance using metrics such as accuracy, precision, and recall.

Clustering changes the story.

In clustering, we do **not** predict an outcome.  
Instead, we try to discover **structure** in the data.

---

### What is clustering trying to do?

Clustering groups observations that are **similar** to each other, based on their features.

The key difference is:

> **There is no target label telling us what the “correct” cluster is.**

So we are not answering:
- “Is this prediction correct?”

We are answering questions such as:
- “Do the data naturally form groups?”
- “If we form groups, are they coherent and interpretable?”
- “Do different algorithms suggest different structures?”

This means evaluation is fundamentally different.

---

### What does “evaluation” mean in clustering?

Because we do not have ground-truth cluster labels, we cannot talk about accuracy in the same way.

Instead, we often rely on:

- **internal measures** (e.g. compactness vs separation),
- **visualisation** (e.g. PCA plots),
- **interpretability** (do the clusters make sense?),
- and stability (do clusters change a lot when we change settings?).

None of these measures are perfect.

That is an important lesson:

> **Clustering results are not “facts”. They are modelling outcomes.**

---

### Why this matters for our datasets

We are using two datasets in different ways:

- **Bank Marketing dataset** (demo):
  - we already know there *is* a target (`y`),  
  - but clustering ignores it,
  - we use it to practice clustering mechanics and interpretation.

- **Online Retail dataset** (your project):
  - clustering can be genuinely meaningful,
  - because there is no natural target,
  - and grouping customers or invoices can help discover behavioural segments.

---

### Key idea to carry forward

> **In clustering, your choices define what “similarity” means.**

That includes decisions about:

- which features to use,
- whether and how to scale them,
- which clustering algorithm to apply,
- and how to interpret the results.

In the next section, we will prepare the Bank Marketing data for clustering, paying close attention to these choices.

---

## Preparing the Data for Clustering

### Selecting Features

Clustering algorithms such as **k-means** rely on numerical distances (typically Euclidean distance).

Therefore:
- we must use **numeric variables**,
- we exclude the target variable `y`,
- and we avoid purely categorical variables for now.

For demonstration, we select a subset of numeric variables:

- `age`
- `duration`
- `campaign`
- `euribor3m`
- `cons.price.idx`
- `cons.conf.idx`

These capture:
- client characteristics,
- campaign intensity,
- economic context.

---

#### R: Selecting Numeric Variables

```{r}
num_vars <- c("age", "duration", "campaign",
              "euribor3m", "cons.price.idx", "cons.conf.idx")

cluster_data <- bank[, num_vars]

head(cluster_data)
```

::: {.callout-note collapse="true"}

#### What this code is doing {.unnumbered}

Creates a vector of selected numeric variables.

Subsets the dataset to include only these columns.

Displays the first few rows.

#### Why this matters:

* k-means operates on numeric input.
* Including categorical variables without encoding would break distance calculations.
* Excluding `y` ensures we are not leaking outcome information.

:::

#### Python: Selecting Numeric Variables

```{python}
num_vars = ["age", "duration", "campaign",
            "euribor3m", "cons.price.idx", "cons.conf.idx"]

cluster_data = bank[num_vars]

cluster_data.head()

```
::: {.callout-note collapse="true"}

#### Why we explicitly define variables

Even if many variables are numeric, we choose them deliberately.

Clustering is sensitive to:

* which dimensions are included,
* how many dimensions are included,
* and whether some dominate others.

Feature choice defines similarity.

:::

---

### Why Scaling Is Not Optional in Clustering

Consider the variables:

* `duration` (measured in seconds),
* `age` (measured in years),
* `cons.price.idx` (around ~93–95),
* `euribor3m` (around ~0–5).

If we compute Euclidean distance directly:

* variables with larger numerical ranges dominate,
* clustering becomes biased toward those dimensions.

This is different from decision trees:

* trees are scale-invariant,
* distance-based methods are not.

> In clustering, scaling changes geometry. Geometry determines clusters.

---

#### R: Standardising the Data

```{r}
cluster_scaled <- scale(cluster_data)

head(cluster_scaled)
```

::: {.callout-note collapse="true"}

#### What `scale()` does

Subtracts the mean from each variable.

* Divides by the standard deviation.
* Produces variables with mean = 0 and standard deviation = 1.

#### Why this matters:

* All features now contribute equally to distance.
* Clustering becomes about relative patterns, not magnitude differences.

:::

#### Python: Standardising the Data

```{python}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
cluster_scaled = scaler.fit_transform(cluster_data)

cluster_scaled[:5]
```

::: {.callout-note collapse="true"}

#### What this code is doing

* Creates a `StandardScaler` object.
* Fits it to the data (learns mean and variance).
* Transforms the data to standardised form.

Important distinction:

* Trees did not require scaling.
* k-means and other distance-based algorithms do.

:::

### Conceptual Checkpoint

Before moving to clustering:

* We selected numeric variables intentionally.
* We excluded the target variable.
* We standardised features to avoid dominance.

These are not technical details. They are modelling decisions.

In the next section, we apply k-means, the most widely used variance-based clustering algorithm.

---

## K-Means — Clustering by Minimising Variance

We now apply the first clustering algorithm: **k-means**.

K-means is one of the simplest and most widely used clustering methods.

Its goal is:

> Partition the data into **k clusters**  
> such that within-cluster variance is minimised.

---

### Conceptual Overview

K-means works by:

1. Choosing `k` initial cluster centres.
2. Assigning each observation to the nearest centre.
3. Updating the centres to the mean of assigned points.
4. Repeating steps 2–3 until convergence.

The algorithm optimises:

> **Within-Cluster Sum of Squares (WCSS)**

That is, it tries to keep clusters compact.

Important assumptions:

- Clusters are roughly spherical.
- Distance is Euclidean.
- `k` must be chosen in advance.

---

### Fitting K-Means (k = 3)

For demonstration, we choose `k = 3`.

This choice is arbitrary for now — we will later discuss how to assess it.

---

#### R: Fitting K-Means

```{r}
set.seed(42)

kmeans_model <- kmeans(
  cluster_scaled,
  centers = 3,
  nstart = 20
)

kmeans_model
```
::: {.callout-note collapse="true"}

#### What this code is doing

* `centers = 3` specifies the number of clusters.
* `nstart = 20` runs the algorithm 20 times with different initialisations.
* The best solution (lowest total within-cluster sum of squares) is kept.

Why this matters:

* K-means can converge to local minima.
* Multiple starts improve stability.

:::

#### Python: Fitting K-Means

```{python}
from sklearn.cluster import KMeans

kmeans_model = KMeans(
    n_clusters=3,
    random_state=42,
    n_init=20
)

kmeans_model.fit(cluster_scaled)
```

::: {.callout-note collapse="true"}

#### What this code is doing

* `n_clusters=3` sets the number of clusters.
* `n_init=20` runs the algorithm multiple times.
* `random_state=42` ensures reproducibility.

Like in R, multiple initialisations reduce instability.

:::




### Inspecting Cluster Sizes

Understanding cluster size distribution is important. Very small clusters may indicate:

* outliers,
* instability,
* or over-segmentation.

#### R: Cluster Sizes

```{r}
table(kmeans_model$cluster)
```

#### Python: Cluster Sizes

```{python}
import numpy as np

np.bincount(kmeans_model.labels_)
```

### Interpretation questions:

* Are clusters roughly balanced?
* Does one cluster dominate?
* Does any cluster appear unusually small?

Remember:

> K-means always assigns every point to a cluster. There is no notion of “noise”.

### Interpreting Cluster Centres

Cluster centres represent the mean feature values of each cluster (in scaled space).

To interpret them meaningfully, we often transform them back to the original scale.

#### R: Viewing Cluster Centres

```{r}
centers_scaled <- kmeans_model$centers
centers_scaled
```


#### Python: Viewing Cluster Centres

```{python}
centers_scaled = kmeans_model.cluster_centers_
centers_scaled
```

If we want to convert back to original scale (Python example):

```{python}
centers_original = scaler.inverse_transform(centers_scaled)
centers_original
```

### How to Interpret Centres

Each row corresponds to a cluster. For each cluster, ask:

* Is average duration higher or lower?
* Is campaign intensity different?
* Are economic indicators different?

These differences describe structural patterns in the data.

But be careful:

> Clusters do not imply causation.
> They reflect geometric grouping under Euclidean distance.

### Conceptual Checkpoint

At this stage:

* We have grouped clients into 3 clusters.
* The grouping is based purely on numeric similarity.
* The target variable y has not influenced clustering.

Next, we must ask:

* Is 3 a reasonable choice for `k`?
* And how do we evaluate cluster quality?

---


## Evaluating Clusters — Heuristics, Not Truth

Unlike supervised learning, clustering does not have a natural “accuracy”.

There is no ground truth telling us:
- whether cluster 1 is correct,
- whether cluster 2 is meaningful,
- or whether 3 clusters are better than 4.

Instead, we use **heuristics** — quantitative signals that help us reason about structure.

Two common tools:

1. The **Elbow Method** (based on variance)
2. The **Silhouette Score** (based on cohesion and separation)

These are guides — not proofs.

---

###  The Elbow Method (Within-Cluster Sum of Squares)

K-means minimises **Within-Cluster Sum of Squares (WCSS)**.

As we increase `k`:
- WCSS always decreases,
- because more clusters mean tighter grouping.

The question is:

> At what point does adding more clusters produce diminishing returns?

---

#### R: Computing WCSS for Multiple k

```{r}
set.seed(42)

wcss <- sapply(1:8, function(k) {
  kmeans(cluster_scaled, centers = k, nstart = 10)$tot.withinss
})

plot(1:8, wcss, type = "b",
     xlab = "Number of Clusters (k)",
     ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method")
```

---

### Python: Computing WCSS for Multiple k

```{python}
import matplotlib.pyplot as plt

wcss = []

for k in range(1, 9):
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    km.fit(cluster_scaled)
    wcss.append(km.inertia_)

plt.plot(range(1, 9), wcss, marker='o')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Within-Cluster Sum of Squares")
plt.title("Elbow Method")
plt.show()
```

::: {.callout-note collapse="true"}

### What we are looking for

* A visible “bend” in the curve.
* After the bend, improvements become smaller.

Important:

* Sometimes the elbow is clear.
* Often it is ambiguous.
* There is rarely a mathematically perfect answer.

:::

---

### Silhouette Score

The silhouette score measures:

* how close each point is to its own cluster (cohesion),
* compared to other clusters (separation).

Values range from:

* **-1 to 1**

Interpretation:

* Close to 1 → well-clustered
* Around 0 → overlapping clusters
* Negative → likely misclassification

---

### R: Silhouette Score

```{r}
library(cluster)

sil <- silhouette(kmeans_model$cluster, dist(cluster_scaled))
mean(sil[, 3])
```

---

### Python: Silhouette Score

```{python}
from sklearn.metrics import silhouette_score

silhouette_score(cluster_scaled, kmeans_model.labels_)
```

::: {.callout-note collapse="true"}

### Important limitation

A higher silhouette score does not guarantee meaningful clusters.

It only measures geometric separation.

Interpretability still requires domain reasoning.

:::

---

### What These Measures Do — and Do Not — Tell Us

These tools:

✔ Help compare different values of `k`
✔ Provide quantitative structure signals
✔ Encourage systematic reasoning

But they do **not**:

✘ Guarantee real-world meaning
✘ Reveal causality
✘ Ensure stability across samples

Clustering always involves interpretation.

---

### Conceptual Checkpoint

At this stage, we have:

* Fit k-means
* Explored cluster sizes
* Compared multiple `k` values
* Measured silhouette score

We now turn to visualisation.

High-dimensional structure is hard to see directly.

In the next section, we use **Principal Component Analysis (PCA)** to visualise cluster structure in two dimensions.

---

## PCA — Visualising High-Dimensional Structure

Clustering was performed in a **6-dimensional space**.

Humans cannot visualise 6 dimensions directly.

To understand cluster structure visually, we use:

> **Principal Component Analysis (PCA)**

PCA reduces dimensionality while preserving as much variance as possible.

Important:

> PCA preserves variance — not cluster separation.

It is a visualisation tool, not a clustering method.

---

### What PCA Does (Conceptually)

PCA:

1. Finds directions of maximum variance.
2. Projects data onto those directions.
3. Orders components by explained variance.

The first two components often capture a large share of total variance.

But:

- High variance does not necessarily mean good cluster separation.
- PCA can distort cluster shapes.

---

### Fitting PCA

We reduce the data to **2 principal components** for visualisation.

---

#### R: PCA

```{r}
pca_model <- prcomp(cluster_scaled)

pca_2d <- as.data.frame(pca_model$x[, 1:2])
pca_2d$cluster <- as.factor(kmeans_model$cluster)

head(pca_2d)
```

---

#### Python: PCA

```{python}
from sklearn.decomposition import PCA

pca_model = PCA(n_components=2)
pca_2d = pca_model.fit_transform(cluster_scaled)

import pandas as pd
pca_df = pd.DataFrame(pca_2d, columns=["PC1", "PC2"])
pca_df["cluster"] = kmeans_model.labels_

pca_df.head()
```

::: {.callout-note collapse="true"}

#### What this code is doing

* Computes principal components.
* Extracts the first two components.
* Attaches cluster labels for visualisation.

Why this matters:

* We can now plot clusters in 2D.
* This does not change the clustering — it only changes the view.

:::

---

### Plotting Clusters in PCA Space

---

#### R: Plotting

```{r}
library(ggplot2)

ggplot(pca_2d, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.6) +
  labs(title = "K-Means Clusters (PCA Projection)")
```

---

#### Python: Plotting

```{python}
import matplotlib.pyplot as plt

plt.scatter(pca_df["PC1"], pca_df["PC2"],
            c=pca_df["cluster"], cmap="viridis", alpha=0.6)

plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("K-Means Clusters (PCA Projection)")
plt.show()
```

---

### Interpreting the Visualisation

When looking at the plot, ask:

* Are clusters well-separated?
* Do they overlap heavily?
* Is separation mostly along PC1 or PC2?
* Does one cluster appear more dispersed?

But remember:

> PCA shows structure in reduced space.
> It may hide or distort higher-dimensional geometry.

Clusters that appear overlapping in 2D may be well-separated in 6D.

---

### Conceptual Checkpoint

So far we have:

* Applied k-means
* Evaluated using elbow and silhouette
* Visualised structure using PCA

Now we ask a deeper question:

> What if clusters are not spherical?
> What if structure is density-based rather than variance-based?

In the next section, we explore **DBSCAN**, a density-based clustering algorithm.

---

---
title: "Week5: Clustering and PCA"
format:
  html:
    toc: true
    number-sections: true
  pdf:
    toc: true
    number-sections: true
execute:
  warning: false
  message: false
  error: false
---

## Loading Data {.unnumbered}

We reuse the same prepared data introduced earlier.

### R: Loading Data and Creating Splits {.unnumbered}

```{r}
library(tidyverse)
library(dplyr)

bank <- read.csv("data/raw/bank-additional.csv", sep = ";", stringsAsFactors = FALSE)


```

### Python: Loading Data and Creating Splits {.unnumbered}

```{python}
import pandas as pd
from sklearn.compose import ColumnTransformer

bank = pd.read_csv("data/raw/bank-additional.csv", sep=";")

```

---

---

---

## From Prediction to Structure

So far in this module, most of our work has been **supervised**:

- we had a target variable,
- we trained models to predict it,
- we evaluated performance using metrics such as accuracy, precision, and recall.

Clustering changes the story.

In clustering, we do **not** predict an outcome.  
Instead, we try to discover **structure** in the data.

---

### What is clustering trying to do?

Clustering groups observations that are **similar** to each other, based on their features.

The key difference is:

> **There is no target label telling us what the “correct” cluster is.**

So we are not answering:
- “Is this prediction correct?”

We are answering questions such as:
- “Do the data naturally form groups?”
- “If we form groups, are they coherent and interpretable?”
- “Do different algorithms suggest different structures?”

This means evaluation is fundamentally different.

---

### What does “evaluation” mean in clustering?

Because we do not have ground-truth cluster labels, we cannot talk about accuracy in the same way.

Instead, we often rely on:

- **internal measures** (e.g. compactness vs separation),
- **visualisation** (e.g. PCA plots),
- **interpretability** (do the clusters make sense?),
- and stability (do clusters change a lot when we change settings?).

None of these measures are perfect.

That is an important lesson:

> **Clustering results are not “facts”. They are modelling outcomes.**

---

### Why this matters for our datasets

We are using two datasets in different ways:

- **Bank Marketing dataset** (demo):
  - we already know there *is* a target (`y`),  
  - but clustering ignores it,
  - we use it to practice clustering mechanics and interpretation.

- **Online Retail dataset** (your project):
  - clustering can be genuinely meaningful,
  - because there is no natural target,
  - and grouping customers or invoices can help discover behavioural segments.

---

### Key idea to carry forward

> **In clustering, your choices define what “similarity” means.**

That includes decisions about:

- which features to use,
- whether and how to scale them,
- which clustering algorithm to apply,
- and how to interpret the results.

In the next section, we will prepare the Bank Marketing data for clustering, paying close attention to these choices.

---

## Preparing the Data for Clustering

### Selecting Features

Clustering algorithms such as **k-means** rely on numerical distances (typically Euclidean distance).

Therefore:
- we must use **numeric variables**,
- we exclude the target variable `y`,
- and we avoid purely categorical variables for now.

For demonstration, we select a subset of numeric variables:

- `age`
- `duration`
- `campaign`
- `euribor3m`
- `cons.price.idx`
- `cons.conf.idx`

These capture:
- client characteristics,
- campaign intensity,
- economic context.

---

#### R: Selecting Numeric Variables

```{r}
num_vars <- c("age", "duration", "campaign",
              "euribor3m", "cons.price.idx", "cons.conf.idx")

cluster_data <- bank[, num_vars]

head(cluster_data)
```

::: {.callout-note collapse="true"}

#### What this code is doing {.unnumbered}

Creates a vector of selected numeric variables.

Subsets the dataset to include only these columns.

Displays the first few rows.

#### Why this matters:

* k-means operates on numeric input.
* Including categorical variables without encoding would break distance calculations.
* Excluding `y` ensures we are not leaking outcome information.

:::

#### Python: Selecting Numeric Variables

```{python}
num_vars = ["age", "duration", "campaign",
            "euribor3m", "cons.price.idx", "cons.conf.idx"]

cluster_data = bank[num_vars]

cluster_data.head()

```
::: {.callout-note collapse="true"}

#### Why we explicitly define variables

Even if many variables are numeric, we choose them deliberately.

Clustering is sensitive to:

* which dimensions are included,
* how many dimensions are included,
* and whether some dominate others.

Feature choice defines similarity.

:::

---

### Why Scaling Is Not Optional in Clustering

Consider the variables:

* `duration` (measured in seconds),
* `age` (measured in years),
* `cons.price.idx` (around ~93–95),
* `euribor3m` (around ~0–5).

If we compute Euclidean distance directly:

* variables with larger numerical ranges dominate,
* clustering becomes biased toward those dimensions.

This is different from decision trees:

* trees are scale-invariant,
* distance-based methods are not.

> In clustering, scaling changes geometry. Geometry determines clusters.

---

#### R: Standardising the Data

```{r}
cluster_scaled <- scale(cluster_data)

head(cluster_scaled)
```

::: {.callout-note collapse="true"}

#### What `scale()` does

Subtracts the mean from each variable.

* Divides by the standard deviation.
* Produces variables with mean = 0 and standard deviation = 1.

#### Why this matters:

* All features now contribute equally to distance.
* Clustering becomes about relative patterns, not magnitude differences.

:::

#### Python: Standardising the Data

```{python}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
cluster_scaled = scaler.fit_transform(cluster_data)

cluster_scaled[:5]
```

::: {.callout-note collapse="true"}

#### What this code is doing

* Creates a `StandardScaler` object.
* Fits it to the data (learns mean and variance).
* Transforms the data to standardised form.

Important distinction:

* Trees did not require scaling.
* k-means and other distance-based algorithms do.

:::

### Conceptual Checkpoint

Before moving to clustering:

* We selected numeric variables intentionally.
* We excluded the target variable.
* We standardised features to avoid dominance.

These are not technical details. They are modelling decisions.

In the next section, we apply k-means, the most widely used variance-based clustering algorithm.

---

## K-Means — Clustering by Minimising Variance

We now apply the first clustering algorithm: **k-means**.

K-means is one of the simplest and most widely used clustering methods.

Its goal is:

> Partition the data into **k clusters**  
> such that within-cluster variance is minimised.

---

### Conceptual Overview

K-means works by:

1. Choosing `k` initial cluster centres.
2. Assigning each observation to the nearest centre.
3. Updating the centres to the mean of assigned points.
4. Repeating steps 2–3 until convergence.

The algorithm optimises:

> **Within-Cluster Sum of Squares (WCSS)**

That is, it tries to keep clusters compact.

Important assumptions:

- Clusters are roughly spherical.
- Distance is Euclidean.
- `k` must be chosen in advance.

---

### Fitting K-Means (k = 3)

For demonstration, we choose `k = 3`.

This choice is arbitrary for now — we will later discuss how to assess it.

---

#### R: Fitting K-Means

```{r}
set.seed(42)

kmeans_model <- kmeans(
  cluster_scaled,
  centers = 3,
  nstart = 20
)

kmeans_model
```
::: {.callout-note collapse="true"}

#### What this code is doing

* `centers = 3` specifies the number of clusters.
* `nstart = 20` runs the algorithm 20 times with different initialisations.
* The best solution (lowest total within-cluster sum of squares) is kept.

Why this matters:

* K-means can converge to local minima.
* Multiple starts improve stability.

:::

#### Python: Fitting K-Means

```{python}
from sklearn.cluster import KMeans

kmeans_model = KMeans(
    n_clusters=3,
    random_state=42,
    n_init=20
)

kmeans_model.fit(cluster_scaled)
```

::: {.callout-note collapse="true"}

#### What this code is doing

* `n_clusters=3` sets the number of clusters.
* `n_init=20` runs the algorithm multiple times.
* `random_state=42` ensures reproducibility.

Like in R, multiple initialisations reduce instability.

:::




### Inspecting Cluster Sizes

Understanding cluster size distribution is important. Very small clusters may indicate:

* outliers,
* instability,
* or over-segmentation.

#### R: Cluster Sizes

```{r}
table(kmeans_model$cluster)
```

#### Python: Cluster Sizes

```{python}
import numpy as np

np.bincount(kmeans_model.labels_)
```

### Interpretation questions:

* Are clusters roughly balanced?
* Does one cluster dominate?
* Does any cluster appear unusually small?

Remember:

> K-means always assigns every point to a cluster. There is no notion of “noise”.

### Interpreting Cluster Centres

Cluster centres represent the mean feature values of each cluster (in scaled space).

To interpret them meaningfully, we often transform them back to the original scale.

#### R: Viewing Cluster Centres

```{r}
centers_scaled <- kmeans_model$centers
centers_scaled
```


#### Python: Viewing Cluster Centres

```{python}
centers_scaled = kmeans_model.cluster_centers_
centers_scaled
```

If we want to convert back to original scale (Python example):

```{python}
centers_original = scaler.inverse_transform(centers_scaled)
centers_original
```

### How to Interpret Centres

Each row corresponds to a cluster. For each cluster, ask:

* Is average duration higher or lower?
* Is campaign intensity different?
* Are economic indicators different?

These differences describe structural patterns in the data.

But be careful:

> Clusters do not imply causation.
> They reflect geometric grouping under Euclidean distance.

### Conceptual Checkpoint

At this stage:

* We have grouped clients into 3 clusters.
* The grouping is based purely on numeric similarity.
* The target variable y has not influenced clustering.

Next, we must ask:

* Is 3 a reasonable choice for `k`?
* And how do we evaluate cluster quality?

---


## Evaluating Clusters — Heuristics, Not Truth

Unlike supervised learning, clustering does not have a natural “accuracy”.

There is no ground truth telling us:
- whether cluster 1 is correct,
- whether cluster 2 is meaningful,
- or whether 3 clusters are better than 4.

Instead, we use **heuristics** — quantitative signals that help us reason about structure.

Two common tools:

1. The **Elbow Method** (based on variance)
2. The **Silhouette Score** (based on cohesion and separation)

These are guides — not proofs.

---

###  The Elbow Method (Within-Cluster Sum of Squares)

K-means minimises **Within-Cluster Sum of Squares (WCSS)**.

As we increase `k`:
- WCSS always decreases,
- because more clusters mean tighter grouping.

The question is:

> At what point does adding more clusters produce diminishing returns?

---

#### R: Computing WCSS for Multiple k

```{r}
set.seed(42)

wcss <- sapply(1:8, function(k) {
  kmeans(cluster_scaled, centers = k, nstart = 10)$tot.withinss
})

plot(1:8, wcss, type = "b",
     xlab = "Number of Clusters (k)",
     ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method")
```

---

### Python: Computing WCSS for Multiple k

```{python}
import matplotlib.pyplot as plt

wcss = []

for k in range(1, 9):
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    km.fit(cluster_scaled)
    wcss.append(km.inertia_)

plt.plot(range(1, 9), wcss, marker='o')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Within-Cluster Sum of Squares")
plt.title("Elbow Method")
plt.show()
```

::: {.callout-note collapse="true"}

### What we are looking for

* A visible “bend” in the curve.
* After the bend, improvements become smaller.

Important:

* Sometimes the elbow is clear.
* Often it is ambiguous.
* There is rarely a mathematically perfect answer.

:::

---

### Silhouette Score

The silhouette score measures:

* how close each point is to its own cluster (cohesion),
* compared to other clusters (separation).

Values range from:

* **-1 to 1**

Interpretation:

* Close to 1 → well-clustered
* Around 0 → overlapping clusters
* Negative → likely misclassification

---

### R: Silhouette Score

```{r}
library(cluster)

sil <- silhouette(kmeans_model$cluster, dist(cluster_scaled))
mean(sil[, 3])
```

---

### Python: Silhouette Score

```{python}
from sklearn.metrics import silhouette_score

silhouette_score(cluster_scaled, kmeans_model.labels_)
```

::: {.callout-note collapse="true"}

### Important limitation

A higher silhouette score does not guarantee meaningful clusters.

It only measures geometric separation.

Interpretability still requires domain reasoning.

:::

---

### What These Measures Do — and Do Not — Tell Us

These tools:

✔ Help compare different values of `k`
✔ Provide quantitative structure signals
✔ Encourage systematic reasoning

But they do **not**:

✘ Guarantee real-world meaning
✘ Reveal causality
✘ Ensure stability across samples

Clustering always involves interpretation.

---

### Conceptual Checkpoint

At this stage, we have:

* Fit k-means
* Explored cluster sizes
* Compared multiple `k` values
* Measured silhouette score

We now turn to visualisation.

High-dimensional structure is hard to see directly.

In the next section, we use **Principal Component Analysis (PCA)** to visualise cluster structure in two dimensions.

---

## PCA — Visualising High-Dimensional Structure

Clustering was performed in a **6-dimensional space**.

Humans cannot visualise 6 dimensions directly.

To understand cluster structure visually, we use:

> **Principal Component Analysis (PCA)**

PCA reduces dimensionality while preserving as much variance as possible.

Important:

> PCA preserves variance — not cluster separation.

It is a visualisation tool, not a clustering method.

---

### What PCA Does (Conceptually)

PCA:

1. Finds directions of maximum variance.
2. Projects data onto those directions.
3. Orders components by explained variance.

The first two components often capture a large share of total variance.

But:

- High variance does not necessarily mean good cluster separation.
- PCA can distort cluster shapes.

---

### Fitting PCA

We reduce the data to **2 principal components** for visualisation.

---

#### R: PCA

```{r}
pca_model <- prcomp(cluster_scaled)

pca_2d <- as.data.frame(pca_model$x[, 1:2])
pca_2d$cluster <- as.factor(kmeans_model$cluster)

head(pca_2d)
```

---

#### Python: PCA

```{python}
from sklearn.decomposition import PCA

pca_model = PCA(n_components=2)
pca_2d = pca_model.fit_transform(cluster_scaled)

import pandas as pd
pca_df = pd.DataFrame(pca_2d, columns=["PC1", "PC2"])
pca_df["cluster"] = kmeans_model.labels_

pca_df.head()
```

::: {.callout-note collapse="true"}

#### What this code is doing

* Computes principal components.
* Extracts the first two components.
* Attaches cluster labels for visualisation.

Why this matters:

* We can now plot clusters in 2D.
* This does not change the clustering — it only changes the view.

:::

---

### Plotting Clusters in PCA Space

---

#### R: Plotting

```{r}
library(ggplot2)

ggplot(pca_2d, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.6) +
  labs(title = "K-Means Clusters (PCA Projection)")
```

---

#### Python: Plotting

```{python}
import matplotlib.pyplot as plt

plt.scatter(pca_df["PC1"], pca_df["PC2"],
            c=pca_df["cluster"], cmap="viridis", alpha=0.6)

plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("K-Means Clusters (PCA Projection)")
plt.show()
```

---

### Interpreting the Visualisation

When looking at the plot, ask:

* Are clusters well-separated?
* Do they overlap heavily?
* Is separation mostly along PC1 or PC2?
* Does one cluster appear more dispersed?

But remember:

> PCA shows structure in reduced space.
> It may hide or distort higher-dimensional geometry.

Clusters that appear overlapping in 2D may be well-separated in 6D.

---

### Conceptual Checkpoint

So far we have:

* Applied k-means
* Evaluated using elbow and silhouette
* Visualised structure using PCA

Now we ask a deeper question:

> What if clusters are not spherical?
> What if structure is density-based rather than variance-based?

In the next section, we explore **DBSCAN**, a density-based clustering algorithm.

---

## DBSCAN — Density-Based Clustering

So far, we used **k-means**, which:

- requires specifying the number of clusters `k`,
- assumes clusters are roughly spherical,
- assigns every point to a cluster.

But what if:

- clusters are irregularly shaped?
- some points are noise?
- density matters more than distance to a centre?

This leads us to **DBSCAN**.

---

### Conceptual Overview

DBSCAN stands for:

> **Density-Based Spatial Clustering of Applications with Noise**

Instead of minimising variance, DBSCAN:

- groups points that are densely packed,
- marks sparse points as noise,
- does not require choosing `k`.

It relies on two key parameters:

- `eps` → neighbourhood radius
- `minPts` / `min_samples` → minimum points to form a dense region

---

### Conceptual Differences: K-Means vs DBSCAN

| K-Means | DBSCAN |
|----------|----------|
| Must choose `k` | No need for `k` |
| Assumes spherical clusters | Can find arbitrary shapes |
| Every point assigned | Some points labelled as noise |
| Based on variance | Based on density |

---

### Applying DBSCAN

We use the same **scaled data**.

This keeps comparison fair.

---

#### R: DBSCAN

```{r}
library(dbscan)

db_model <- dbscan(cluster_scaled, eps = 0.8, minPts = 10)

table(db_model$cluster)
```

::: {.callout-note collapse="true"}

### What this code is doing

* `eps` defines the radius of neighbourhood.
* `minPts` defines how many points are required to form a dense region.
* Points that do not meet density requirements are labelled as 0 (noise).

Why this matters:

* DBSCAN does not force every point into a cluster.
* Parameter choice strongly influences results.

:::

---

#### Python: DBSCAN

```{python}
from sklearn.cluster import DBSCAN

db_model = DBSCAN(eps=0.8, min_samples=10)
db_labels = db_model.fit_predict(cluster_scaled)

import numpy as np
np.unique(db_labels, return_counts=True)
```

::: {.callout-note collapse="true"}

### What to notice

* Cluster labels include `-1` for noise points.
* Number of clusters emerges from the data.
* Small changes in `eps` can dramatically change results.

:::

---

### Visualising DBSCAN Results (PCA Projection)

We again use PCA for 2D visualisation.

---

#### R: PCA Plot with DBSCAN

```{r}
pca_2d$db_cluster <- as.factor(db_model$cluster)

ggplot(pca_2d, aes(x = PC1, y = PC2, color = db_cluster)) +
  geom_point(alpha = 0.6) +
  labs(title = "DBSCAN Clusters (PCA Projection)")
```

---

#### Python: PCA Plot with DBSCAN

```{python}
pca_df["db_cluster"] = db_labels

plt.scatter(pca_df["PC1"], pca_df["PC2"],
            c=pca_df["db_cluster"], cmap="viridis", alpha=0.6)

plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("DBSCAN Clusters (PCA Projection)")
plt.show()
```

---

### Interpreting DBSCAN Output

Key questions:

* How many clusters were found?
* How many points were labelled as noise?
* Are clusters shaped differently than k-means clusters?

Important:

> DBSCAN does not try to minimise variance.
> It tries to detect dense regions.

This means:

* Results may differ substantially from k-means.
* There is no single “correct” clustering.

---

### Conceptual Checkpoint

At this point, we have seen:

* Variance-based clustering (k-means)
* Density-based clustering (DBSCAN)
* Dimensional projection (PCA)

The natural question is:

> Can we build clusters without fixing parameters like `k` or `eps` in advance?

In the next section, we explore **hierarchical clustering**, which builds a tree of possible groupings.

> Section Missing

In the final section, we connect this to your **Online Retail project dataset** and discuss what clustering means in that context.

---

## Connecting Clustering to Your Online Retail Project

So far, we used the **Bank Marketing dataset** to demonstrate clustering mechanics.

But there is an important observation:

> The Bank Marketing dataset already has a target (`y`).

Clustering it is therefore somewhat artificial —  
we are ignoring known outcome information.

In contrast, your **Online Retail dataset** does not have a natural prediction target.

This makes clustering potentially much more meaningful.

---

### The First Decision: What Is the Unit of Analysis?

Before clustering Retail data, you must decide:

> What exactly are you clustering?

Possible choices:

- Individual transaction rows?
- Invoices?
- Customers?

This is not a technical decision —  
it is a modelling decision.

For example:

- Clustering transactions may reveal purchase types.
- Clustering customers may reveal behavioural segments.

You must justify your choice.

---

### Feature Engineering Comes First

Clustering is extremely sensitive to representation.

For customer-level clustering, common features might include:

- Total spend
- Average basket value
- Number of transactions
- Recency (days since last purchase)
- Number of distinct products

These are not provided directly.

They must be constructed.

This connects directly to what you practised in Week 1:
- grouping,
- aggregation,
- summarisation.

---

### Scaling Is Essential

Retail features often differ greatly in scale:

- total spend may be in thousands,
- number of transactions may be small integers,
- recency may be in days.

Without scaling:

- large monetary values dominate,
- clustering becomes distorted.

Unlike trees:

> Clustering requires careful scaling decisions.

---

### Choosing an Algorithm

You may choose (at least two):

- K-means (simple, fast, interpretable centres),
- DBSCAN (detect dense purchasing groups, identify outliers),
- Hierarchical clustering (understand nested segmentation).

But remember:

> Different algorithms imply different assumptions about structure.

There is no universally correct choice.

---

### Interpreting Clusters

Clustering does not produce “answers”.

It produces groups.

Your responsibility is to interpret:

- What distinguishes Cluster 1 from Cluster 2?
- Are high-spend customers separated?
- Are infrequent buyers grouped together?
- Are some clusters small and possibly noise?

Interpretation is more important than the algorithm itself.

---

#### What Clustering Is Not

Clustering:

- does not prove causation,
- does not guarantee business usefulness,
- does not discover “true” segments automatically.

Clusters are:

> Structures induced by modelling assumptions.

---

### Final Reflection

Across this demo, we saw:

- How scaling defines geometry,
- How k-means minimises variance,
- How DBSCAN detects density,
- How hierarchical clustering builds nested structure,
- How PCA helps visualise high-dimensional patterns.

The consistent theme is:

> Representation defines similarity.  
> Similarity defines clusters.  
> Clusters require interpretation.

In your project work, the most important questions will not be:

- “What value of k did you choose?”

But rather:

- “Why did you choose these features?”
- “Why did you scale this way?”
- “What do the clusters represent?”
- “What are the limitations?”

That is where genuine understanding lies.


---


